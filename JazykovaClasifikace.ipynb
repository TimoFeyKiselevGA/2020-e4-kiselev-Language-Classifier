{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JazykovaClasifikace.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "P6aIaJ1anTe5"
      ],
      "authorship_tag": "ABX9TyM/flc8iLCYqRytPg4feKSp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TimoFeyKiselevGA/Language-Classifier/blob/main/JazykovaClasifikace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-mN8Xc3iIVT"
      },
      "source": [
        "# **Part 1, downloading and extracting texts** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1lk-zmwiqfz"
      },
      "source": [
        "## Imports and"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YOojf48KxCQ",
        "outputId": "7374dff1-57eb-49c5-d653-acdf400af930"
      },
      "source": [
        "import xml.etree.ElementTree as etree\n",
        "import codecs\n",
        "import csv\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import re\n",
        "%load_ext google.colab.data_table\n",
        "import os\n",
        "import gc\n",
        "import psutil\n",
        "\n",
        "pd.set_option('display.max_colwidth', -1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ey-XvldiyLr"
      },
      "source": [
        "## Downloading texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRu5E6J63wnX"
      },
      "source": [
        "languages_to_download = {\n",
        "  \"CZ\": True,\n",
        "  \"RU\": True,\n",
        "  \"FR\": True,\n",
        "  \"DE\": True,\n",
        "  \"EN\": True,\n",
        "  \"PL\": True,\n",
        "  \"IT\": True,\n",
        "  \"JA\": True,\n",
        "  \"UK\": True,\n",
        "  \"AR\": True,\n",
        "  \"FI\": True,\n",
        "  \"BG\": True,\n",
        "}\n",
        "\n",
        "languages_links = {\n",
        "    \n",
        "    \"CZ\": \"https://dumps.wikimedia.org/cswiki/20210120/cswiki-20210120-pages-articles.xml.bz2\",\n",
        "    \"RU\": \"https://dumps.wikimedia.org/ruwiki/20210101/ruwiki-20210101-pages-articles-multistream5.xml-p3835773p5335772.bz2\",\n",
        "    \"FR\": \"https://dumps.wikimedia.org/frwiki/20210101/frwiki-20210101-pages-articles-multistream2.xml-p306135p1050822.bz2\",\n",
        "    \"DE\": \"https://dumps.wikimedia.org/dewiki/20210101/dewiki-20210101-pages-articles-multistream6.xml-p9261245p10761244.bz2\",\n",
        "    \"EN\": \"https://dumps.wikimedia.org/enwiki/20210101/enwiki-20210101-pages-articles-multistream9.xml-p2936261p4045402.bz2\",\n",
        "    \"PL\": \"https://dumps.wikimedia.org/plwiki/20210101/plwiki-20210101-pages-articles-multistream4.xml-p1199519p2047892.bz2\",\n",
        "    \"IT\": \"https://dumps.wikimedia.org/itwiki/20210101/itwiki-20210101-pages-articles-multistream4.xml-p2206775p3593336.bz2\",\n",
        "    \"JA\": \"https://dumps.wikimedia.org/jawiki/20210101/jawiki-20210101-pages-articles-multistream5.xml-p1721647p2807947.bz2\",\n",
        "    \"UK\": \"https://dumps.wikimedia.org/ukwiki/20210120/ukwiki-20210120-pages-articles-multistream4.xml-p987986p1674442.bz2\",\n",
        "    \"AR\": \"https://dumps.wikimedia.org/arwiki/20210120/arwiki-20210120-pages-articles-multistream4.xml-p2482316p3982315.bz2\",\n",
        "    \"FI\": \"https://dumps.wikimedia.org/fiwiki/20210120/fiwiki-20210120-pages-articles-multistream.xml.bz2\",\n",
        "    \"BG\": \"https://dumps.wikimedia.org/bgwiki/20210120/bgwiki-20210120-pages-articles-multistream.xml.bz2\",\n",
        "}\n",
        "\n",
        "languages = ['CZ', 'RU', 'FR', 'DE', 'EN', 'PL', 'IT', 'JA', \"UK\", \"AR\", \"FI\", \"BG\"]\n",
        "languages_to_process = {\n",
        "  \"CZ\": True,\n",
        "  \"RU\": True,\n",
        "  \"FR\": True,\n",
        "  \"DE\": True,\n",
        "  \"EN\": True,\n",
        "  \"PL\": True,\n",
        "  \"IT\": True,\n",
        "  \"JA\": True,\n",
        "  \"UK\": True,\n",
        "  \"AR\": True,\n",
        "  \"FI\": True,\n",
        "  \"BG\": True,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiFuGtTTQ042"
      },
      "source": [
        "def rename(s):\n",
        "    return languages_links[s].split(\"/\")[len(languages_links[s].split(\"/\"))-1].split(\".\")[0]+ \".\" +languages_links[s].split(\"/\")[len(languages_links[s].split(\"/\"))-1].split(\".\")[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uw3Q5CnA5G3f",
        "outputId": "a81a268c-bf77-4832-d281-2b1bc633f81b"
      },
      "source": [
        "for lang in languages:\n",
        "    if languages_to_download[lang]:\n",
        "        print(\"downloading \" + lang)\n",
        "        os.system(\"wget \" + languages_links[lang])\n",
        "        print(\"extracting \" + lang)\n",
        "        os.system(\"bzip2 -d ./ \" + languages_links[lang].split(\"/\")[len(languages_links[lang].split(\"/\"))-1])\n",
        "        os.system(\"mv \" + rename(lang) + \" \" + languages_links[lang].split(\"/\")[len(languages_links[lang].split(\"/\"))-1].split(\".\")[0] + \".xml\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading CZ\n",
            "extracting CZ\n",
            "downloading RU\n",
            "extracting RU\n",
            "downloading FR\n",
            "extracting FR\n",
            "downloading DE\n",
            "extracting DE\n",
            "downloading EN\n",
            "extracting EN\n",
            "downloading PL\n",
            "extracting PL\n",
            "downloading IT\n",
            "extracting IT\n",
            "downloading JA\n",
            "extracting JA\n",
            "downloading UK\n",
            "extracting UK\n",
            "downloading AR\n",
            "extracting AR\n",
            "downloading FI\n",
            "extracting FI\n",
            "downloading BG\n",
            "extracting BG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuco0Gr-jtNW"
      },
      "source": [
        "## Creating pkl files with texts without most special chars. from xmls."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOPR-DtIdlJR"
      },
      "source": [
        "def strip_tag_name(t):\n",
        "    idx = k = t.rfind(\"}\")\n",
        "    if idx != -1:\n",
        "        t = t[idx + 1:]\n",
        "    return t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JuCrnVngyfE"
      },
      "source": [
        "def articles_to_csv(totalCount):\n",
        "    with codecs.open(pathTexts, \"w\", ENCODING) as textsFH:\n",
        "        textWriter = csv.writer(textsFH, quoting=csv.QUOTE_MINIMAL)\n",
        "        textWriter.writerow(['text'])\n",
        "        for event, elem in etree.iterparse(pathWikiXML, events=('start', 'end')):\n",
        "            tname = strip_tag_name(elem.tag)\n",
        "            if event == 'start':\n",
        "                if tname == 'page':\n",
        "                    title = ''\n",
        "                    id = -1\n",
        "                    redirect = ''\n",
        "                    inrevision = False\n",
        "                    ns = 0\n",
        "                    text = ''\n",
        "                elif tname == 'text':\n",
        "                    text = elem.text\n",
        "                    textWriter.writerow([text])\n",
        "                elif tname == 'page':\n",
        "                    totalCount += 1\n",
        "                if totalCount > 1 and (totalCount % 100000) == 0:\n",
        "                    print(\"{:,}\".format(totalCount))\n",
        "            elem.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ip12b-8RUsLV",
        "outputId": "2e8f006f-4123-4efe-edb6-0ec631f3e263"
      },
      "source": [
        "for lang in languages:\n",
        "    if languages_to_process[lang]:\n",
        "        print(lang)\n",
        "        PATH_WIKI_XML = './'\n",
        "        FILENAME_WIKI = languages_links[lang].split(\"/\")[len(languages_links[lang].split(\"/\"))-1].split('.')[0] + '.xml'\n",
        "        texts_template = 'temp.csv'\n",
        "        ENCODING = \"utf-8\"\n",
        "        pathWikiXML = os.path.join(PATH_WIKI_XML, FILENAME_WIKI)\n",
        "        pathTexts = os.path.join(PATH_WIKI_XML, texts_template)\n",
        "        totalCount = 0\n",
        "        articleCount = 0\n",
        "        title = None\n",
        "        articles_to_csv(totalCount)\n",
        "        a = pd.read_csv('temp.csv')\n",
        "        a = a.dropna()\n",
        "        a['text'] = a['text'].str.replace(',|\\$|\\{|\\}|\\-|\\+|\\[|\\]|\\\\n|\\<|\\>|\\\"|\\=|\\:|\\;|\\*|\\||\\#|\\$|\\(|\\)|\\/|\\.|\\–|\\_|\\«|\\»|\\—|\\!|\\&|\\?|\\`|\\~|\\%|\\@|\\¡|\\™|\\£|\\¢|\\∞|\\§|\\¶|\\•|\\ª|\\º|\\–|\\≠|\\«|\\æ|\\÷|\\≥|\\≤|\\÷|\\»|\\\\|\\^|\\ˆ|\\|\\\\|\\||\\）|\\（|\\、|\\'', ' ')\n",
        "        a['text'] = a['text'].str.replace('\\d+', '')\n",
        "        a['text'] = a['text'].str.replace('\\s\\s+' , ' ')\n",
        "        a = a.drop([a.index[0]])\n",
        "        a = a.reset_index(drop=True)\n",
        "        a['text'] = a['text'].str.lower()\n",
        "        a.to_pickle(\"./texts_\" + lang +  \".pkl\")\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CZ\n",
            "RU\n",
            "FR\n",
            "DE\n",
            "EN\n",
            "PL\n",
            "IT\n",
            "JA\n",
            "UK\n",
            "AR\n",
            "FI\n",
            "BG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY2d3zSHC85E"
      },
      "source": [
        "%reset -f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-x6PwcnkRXa"
      },
      "source": [
        "# **Part 2: Dataset preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMSTAFOOknpp"
      },
      "source": [
        "## Imports and settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKHpKVe21ZzK",
        "outputId": "a59d2661-5836-49bf-827d-985810eedfda"
      },
      "source": [
        "import xml.etree.ElementTree as etree\n",
        "import codecs\n",
        "import csv\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import re\n",
        "%load_ext google.colab.data_table\n",
        "import os\n",
        "import gc\n",
        "import psutil\n",
        "import numpy as np\n",
        "import random\n",
        "#random seeds\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "languages = ['CZ', 'RU', 'FR', 'DE', 'EN', 'PL', 'IT', 'JA', \"UK\", \"AR\", \"FI\", \"BG\"]\n",
        "languages_to_process = {\n",
        "  \"CZ\": True,\n",
        "  \"RU\": True,\n",
        "  \"FR\": True,\n",
        "  \"DE\": True,\n",
        "  \"EN\": True,\n",
        "  \"PL\": True,\n",
        "  \"IT\": True,\n",
        "  \"JA\": True,\n",
        "  \"UK\": True,\n",
        "  \"AR\": True,\n",
        "  \"FI\": True,\n",
        "  \"BG\": True,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r5l2xqgk8N7"
      },
      "source": [
        "## Dataset creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHfk50TfuGwH"
      },
      "source": [
        "def chunkstring(string, length):\n",
        "    return re.findall('.{%d}' % length, string)\n",
        "\n",
        "def toPickleLoadList(string, textList):\n",
        "    file_name = \"texts_\" + string + \"_prep.pkl\"\n",
        "    open_file = open(file_name, \"wb\")\n",
        "    pickle.dump(textList, open_file)\n",
        "    open_file.close()\n",
        "    print(\"done \" + lang)\n",
        "\n",
        "def fromPickleLoadList(string):\n",
        "    file_name = \"texts_\" + string + \"_prep.pkl\"\n",
        "    open_file = open(file_name, \"rb\")\n",
        "    loaded_list = pickle.load(open_file)\n",
        "    open_file.close()\n",
        "    return loaded_list\n",
        "\n",
        "def chunkDataFrames(lang):\n",
        "    print(\"start \" + lang)\n",
        "    df = pd.read_pickle(\"./texts_\"+ lang + \".pkl\")\n",
        "    df = df[df['text'].apply(lambda x: len(x)>100)]\n",
        "    # df = df[~df.text.str.startswith(' redirect', na=True)]\n",
        "    df.text = df.text.replace('\\s+', ' ', regex=True)\n",
        "    df = df.text.str.cat(sep='')\n",
        "    textList = chunkstring(df, 200)\n",
        "    toPickleLoadList(lang, textList)\n",
        "    del df;  gc.collect()\n",
        "    del textList;  gc.collect()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zoaj3NtCUsQ0",
        "outputId": "248e6034-911a-482b-e42f-1888e8ad96fd"
      },
      "source": [
        "for lang in languages:\n",
        "    if languages_to_process[lang]:\n",
        "        chunkDataFrames(lang)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start CZ\n",
            "done CZ\n",
            "start RU\n",
            "done RU\n",
            "start FR\n",
            "done FR\n",
            "start DE\n",
            "done DE\n",
            "start EN\n",
            "done EN\n",
            "start PL\n",
            "done PL\n",
            "start IT\n",
            "done IT\n",
            "start JA\n",
            "done JA\n",
            "start UK\n",
            "done UK\n",
            "start AR\n",
            "done AR\n",
            "start FI\n",
            "done FI\n",
            "start BG\n",
            "done BG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSAg50zD22fS"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "text_to_tokens = []\n",
        "id_lang = []\n",
        "id = 0\n",
        "delka = 100000\n",
        "\n",
        "for lang in languages:\n",
        "    df = shuffle(fromPickleLoadList(lang))\n",
        "    df = df[:delka]\n",
        "    id_lang = id_lang + [id] * delka\n",
        "    id = id + 1\n",
        "    text_to_tokens = text_to_tokens + df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUZJFcaR23sJ"
      },
      "source": [
        "data_tuples = list(zip(text_to_tokens,id_lang))\n",
        "df = pd.DataFrame(data_tuples, columns=['X','Y'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnOvuHQu3t5Z"
      },
      "source": [
        "df = shuffle(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "Jq5-HUCm2eAN",
        "outputId": "fb1c7b10-9c6d-4d73-9ef0-36f1fc783b19"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 843521,\n            'f': \"843521\",\n        },\n\"\\u043e\\u0432\\u043d\\u043e\\u0433\\u043e \\u043a\\u043e\\u0440\\u043e\\u043b\\u044f \\u0456\\u0440\\u043b\\u0430\\u043d\\u0434\\u0456\\u0457 \\u0456\\u0440\\u0435\\u0440\\u0435\\u043e \\u0456\\u0440\\u043b nbsp irereo \\u0449\\u043e \\u0431\\u0443\\u0432 \\u0432\\u0431\\u0438\\u0432\\u0446\\u0435\\u044e \\u0439\\u043e\\u0433\\u043e \\u0431\\u0430\\u0442\\u044c\\u043a\\u0430 \\u0446\\u0435 \\u0441\\u0442\\u0430\\u043b\\u043e\\u0441\\u044f \\u043d\\u0430 \\u0442\\u0435\\u0440\\u0438\\u0442\\u043e\\u0440\\u0456\\u0457 \\u0432\\u0430\\u0441\\u0430\\u043b\\u044c\\u043d\\u043e\\u0433\\u043e \\u043a\\u043e\\u0440\\u043e\\u043b\\u0456\\u0432\\u0441\\u0442\\u0432\\u0430 \\u0443\\u043b\\u0430\\u0434 \\u043e\\u043b\\u044c\\u0441\\u0442\\u0435\\u0440 \\u043f\\u0440\\u0430\\u0432\\u0438\\u0432 \\u0456\\u0440\\u043b\\u0430\\u043d\\u0434\\u0456\\u0454\\u044e \\u043f\\u0440\\u043e\\u0442\\u044f\\u0433\\u043e\\u043c \\u043e\\u0434\\u0438\\u043d\\u0430\\u0434\\u0446\\u044f\\u0442\\u0438 \\u0440\\u043e\\u043a\\u0456\\u0432 \\u0431\\u0443\\u0432 \\u0432\\u0431\\u0438\\u0442\\u0438\\u0439 \\u0441\\u0438\\u043d\\u043e\\u043c \\u0456\\u0440\\u0435\\u0440\\u0435\\u043e \",\n{\n            'v': 8,\n            'f': \"8\",\n        }],\n [{\n            'v': 882105,\n            'f': \"882105\",\n        },\n\"bsp \\u0443\\u043a\\u0440\\u0430\\u0457\\u043d\\u0441\\u044c\\u043a\\u0435 \\u043f\\u0440\\u0456\\u0437\\u0432\\u0438\\u0449\\u0435 \\u044f\\u043a\\u0435 \\u043c\\u0430\\u0454 \\u043a\\u043e\\u043c\\u043f\\u043b\\u0435\\u043a\\u0441\\u043d\\u0443 \\u0435\\u0442\\u0438\\u043c\\u043e\\u043b\\u043e\\u0433\\u0456\\u044e \\u0432\\u043e\\u043d\\u043e \\u043f\\u043e\\u0445\\u043e\\u0434\\u0438\\u0442\\u044c \\u0432\\u0456\\u0434 \\u043a\\u0456\\u043b\\u044c\\u043a\\u043e\\u0445 \\u0456\\u043c\\u0435\\u043d \\u043d\\u0430 \\u0437\\u0440\\u0430\\u0437\\u043e\\u043a krisant kristofor kristijan \\u043a\\u0440\\u0438\\u0441\\u043a\\u0435\\u043d\\u0442 \\u043a\\u0440\\u0435\\u0441\\u0438\\u043c\\u0438\\u0440 kpecoje \\u043a\\u0438\\u0440\\u0441\\u0430\\u043d\\u0442\\u0456\\u0439 \\u0442\\u0430\\u043a\\u043e\\u0436 \\u0454 \\u043f\\u0440\\u0456\\u0437\\u0432\\u0438\\u0449\\u0430 \\u0432\\u0456\\u0434 \\u043f\\u043e\\u043b\\u044c\\u0441\\u044c\\u043a\\u0438\\u0445 \\u043e\\u0441\\u043d\\u043e\\u0432 kres\",\n{\n            'v': 8,\n            'f': \"8\",\n        }],\n [{\n            'v': 192632,\n            'f': \"192632\",\n        },\n\"\\u0437\\u044b\\u043a\\u0430 \\u043c \\u0444\\u0430\\u0441\\u043c\\u0435\\u0440\\u0430 \\u0432\\u043e\\u0437\\u0432\\u043e\\u0434\\u0438\\u0442 \\u043f\\u0440\\u043e\\u0438\\u0441\\u0445\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0435 \\u0441\\u043b\\u043e\\u0432\\u0430 \\u043f\\u043b\\u0451\\u0441 \\u043a \\u0434\\u0440\\u0435\\u0432\\u043d\\u0435\\u0440\\u0443\\u0441\\u0441\\u043a\\u043e\\u043c\\u0443 \\u0441\\u043b\\u043e\\u0432\\u0443 \\u043f\\u043b\\u0435\\u0441\\u044a \\u0433\\u043b\\u0443\\u0431\\u043e\\u043a\\u043e\\u0435 \\u043c\\u0435\\u0441\\u0442\\u043e \\u0432 \\u0432\\u043e\\u0434\\u0435 \\u043e\\u0437\\u0435\\u0440\\u0435 \\u0441\\u0435\\u043c\\u0430\\u043d\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438 \\u043d\\u0430\\u0438\\u0431\\u043e\\u043b\\u0435\\u0435 \\u0432\\u0435\\u0440\\u043e\\u044f\\u0442\\u043d\\u043e \\u0440\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e \\u0441\\u043e \\u0441\\u043b\\u0430\\u0432\\u044f\\u043d\\u0441\\u043a\\u0438\\u043c\\u0438 \\u043a\\u043e\\u0440\\u043d\\u044f\\u043c\\u0438 \\u0440les\\u043e \\u0440lets\\u043e \\u0433\\u0440\\u0435\\u0447 \\u03c0\\u03bb\\u03ac\\u03c4\\u03bf\\u03c2 \\u043f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\",\n{\n            'v': 1,\n            'f': \"1\",\n        }],\n [{\n            'v': 80787,\n            'f': \"80787\",\n        },\n\"usindia net results town php stad a state datum archivace nedostupn\\u00e9 ne ref reference references extern\\u00ed odkazy commonscat itanagar indie port\\u00e1ly indie autoritn\\u00ed data kategorie geografie arun\\u00e1\\u010dalprad\\u00e9\",\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 844187,\n            'f': \"844187\",\n        },\n\"\\u0430\\u0434\\u0435\\u043c\\u0456\\u0447\\u043d\\u0456\\u0439 \\u043f\\u0440\\u0430\\u0446\\u0456 \\u0433\\u0440\\u0430\\u043c\\u043c\\u0430\\u0442\\u0438\\u043a\\u0430 \\u0441\\u043e\\u0432\\u0440\\u0435\\u043c\\u0435\\u043d\\u043d\\u043e\\u0433\\u043e \\u0440\\u0443\\u0441\\u0441\\u043a\\u043e\\u0433\\u043e \\u043b\\u0438\\u0442\\u0435\\u0440\\u0430\\u0442\\u0443\\u0440\\u043d\\u043e\\u0433\\u043e \\u044f\\u0437\\u044b\\u043a\\u0430 \\u0440\\u043e\\u043a\\u0443 \\u043f\\u043e\\u044f\\u0441\\u043d\\u044e\\u044e\\u0442\\u044c\\u0441\\u044f \\u043e\\u0441\\u043d\\u043e\\u0432\\u043d\\u0456 \\u0442\\u0435\\u0440\\u043c\\u0456\\u043d\\u0438 \\u0437\\u0432\\u0443\\u0436\\u0435\\u043d\\u043e\\u044e \\u043f\\u0430\\u0440\\u0430\\u0434\\u0438\\u0433\\u043c\\u043e\\u044e \\u043d\\u0430\\u0437\\u0438\\u0432\\u0430\\u0454\\u0442\\u044c\\u0441\\u044f \\u043f\\u0430\\u0440\\u0430\\u0434\\u0438\\u0433\\u043c\\u0430 \\u0440\\u0435\\u043f\\u0440\\u0435\\u0437\\u0435\\u043d\\u0442\\u043e\\u0432\\u0430\\u043d\\u0430 \\u0441\\u0438\\u043d\\u0442\\u0435\\u0442\\u0438\\u0447\\u043d\\u0438\\u043c\\u0438 \\u0444\\u043e\\u0440\\u043c\\u0430\\u043c\\u0438 \\u044f\\u043a\\u0456 \\u0432\\u0438\\u0440\\u0430\\u0436\\u0430\\u044e\\u0442\\u044c \\u043e\\u0441\\u043d\\u043e\\u0432\\u043d\",\n{\n            'v': 8,\n            'f': \"8\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"string\", \"X\"], [\"number\", \"Y\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>843521</th>\n",
              "      <td>овного короля ірландії іререо ірл nbsp irereo що був вбивцею його батька це сталося на території васального королівства улад ольстер правив ірландією протягом одинадцяти років був вбитий сином іререо</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>882105</th>\n",
              "      <td>bsp українське прізвище яке має комплексну етимологію воно походить від кількох імен на зразок krisant kristofor kristijan крискент кресимир kpecoje кирсантій також є прізвища від польських основ kres</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192632</th>\n",
              "      <td>зыка м фасмера возводит происхождение слова плёс к древнерусскому слову плесъ глубокое место в воде озере семантически наиболее вероятно родство со славянскими корнями рlesо рletsо греч πλάτος произво</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80787</th>\n",
              "      <td>usindia net results town php stad a state datum archivace nedostupné ne ref reference references externí odkazy commonscat itanagar indie portály indie autoritní data kategorie geografie arunáčalpradé</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>844187</th>\n",
              "      <td>адемічній праці грамматика современного русского литературного языка року пояснюються основні терміни звуженою парадигмою називається парадигма репрезентована синтетичними формами які виражають основн</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                               X  Y\n",
              "843521  овного короля ірландії іререо ірл nbsp irereo що був вбивцею його батька це сталося на території васального королівства улад ольстер правив ірландією протягом одинадцяти років був вбитий сином іререо   8\n",
              "882105  bsp українське прізвище яке має комплексну етимологію воно походить від кількох імен на зразок krisant kristofor kristijan крискент кресимир kpecoje кирсантій також є прізвища від польських основ kres  8\n",
              "192632  зыка м фасмера возводит происхождение слова плёс к древнерусскому слову плесъ глубокое место в воде озере семантически наиболее вероятно родство со славянскими корнями рlesо рletsо греч πλάτος произво  1\n",
              "80787   usindia net results town php stad a state datum archivace nedostupné ne ref reference references externí odkazy commonscat itanagar indie portály indie autoritní data kategorie geografie arunáčalpradé  0\n",
              "844187  адемічній праці грамматика современного русского литературного языка року пояснюються основні терміни звуженою парадигмою називається парадигма репрезентована синтетичними формами які виражають основн  8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73LzfF4MlJK3"
      },
      "source": [
        "# Part 3: Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl-5id2CleXj"
      },
      "source": [
        "## Imports and settings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkDJCvxt8BsX"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(df, test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXmkpo5X4n6I"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUO_CLzElweI"
      },
      "source": [
        "## Model creation and training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkjNu2mk42cr"
      },
      "source": [
        "def simple_tokenizer(txt, min_len=2):\n",
        "    all_tokens = re.compile(r'[\\w\\d]+').findall(txt)\n",
        "    all_tokens = [x.lower() for x in all_tokens]\n",
        "    return [token for token in all_tokens if len(token) >= min_len]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqQqkh_K4oAY"
      },
      "source": [
        "sklearn_pipeline = Pipeline((('vect', TfidfVectorizer(tokenizer=simple_tokenizer,\n",
        "                                                      max_df=0.8,\n",
        "                                                      min_df=50)),\n",
        "                             ('cls', LogisticRegression())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NK7ZUCei4oH3",
        "outputId": "ecc4f6ef-1f57-4f3d-b305-751bf11d3b81"
      },
      "source": [
        "sklearn_pipeline.fit(train['X'].to_numpy(), train['Y'].to_numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=0.8, max_features=None,\n",
              "                                 min_df=50, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='...w+\\\\b',\n",
              "                                 tokenizer=<function simple_tokenizer at 0x7f6f8c35e1e0>,\n",
              "                                 use_idf=True, vocabulary=None)),\n",
              "                ('cls',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=100,\n",
              "                                    multi_class='auto', n_jobs=None,\n",
              "                                    penalty='l2', random_state=None,\n",
              "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCDsuy8I4oOV"
      },
      "source": [
        "sklearn_train_pred = sklearn_pipeline.predict_proba(train['X'].to_numpy())\n",
        "sklearn_train_loss = F.cross_entropy(torch.from_numpy(sklearn_train_pred),\n",
        "                                                 torch.from_numpy(train['Y'].to_numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56-lC6Y36Hqf",
        "outputId": "27c2b71f-9ed8-43a8-efd0-d3f451b525a8"
      },
      "source": [
        "print('train loss', float(sklearn_train_loss))\n",
        "print('acc', accuracy_score(train['Y'], sklearn_train_pred.argmax(-1)))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss 1.688278924455967\n",
            "acc 0.969027380952381\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F96hEHY6Hxz"
      },
      "source": [
        "sklearn_test_pred = sklearn_pipeline.predict_proba(test['X'].to_numpy())\n",
        "sklearn_test_loss = F.cross_entropy(torch.from_numpy(sklearn_test_pred),\n",
        "                                                torch.from_numpy(test['Y'].to_numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "6QQfqLHY6IC3",
        "outputId": "97792f62-4d35-48b5-9d2b-82c4e1483fb8"
      },
      "source": [
        "print('Val loss', float(sklearn_test_loss))\n",
        "print('acc', accuracy_score(test['Y'], sklearn_test_pred.argmax(-1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-9c9e827c9529>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Val loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msklearn_test_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msklearn_test_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sklearn_test_loss' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "S7iRLNRL7pnC",
        "outputId": "61ba62c0-ef80-4700-c747-7f5246605d35"
      },
      "source": [
        "pokus = ['hilft zu lernen oder.', 'no jo ale to neni  ', ' s a song by the', 'Существуют оба потом']\n",
        "for i in range(0, len(pokus)):\n",
        "    print(languages[sklearn_pipeline.predict_proba(pokus).argmax(-1)[i]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-833a93c02571>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpokus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'hilft zu lernen oder.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'no jo ale to neni  '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' s a song by the'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Существуют оба потом'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpokus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msklearn_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpokus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'sklearn_pipeline' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiEP5jaz9WQg"
      },
      "source": [
        "languages = ['CZ', 'RU', 'FR', 'DE', 'EN', 'PL', 'IT', 'JA', \"UK\", \"AR\", \"FI\", \"BG\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ff2Kkxp99WXe"
      },
      "source": [
        "pickle.dump(sklearn_pipeline, open(\"model.pkl\", 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jksyQrdz9WgY"
      },
      "source": [
        "loaded_model = pickle.load(open(\"model1.pkl\", 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd3ccbN22ZzU",
        "outputId": "6c35a40b-4b42-4a23-a3ae-ed65bbf4d4d6"
      },
      "source": [
        "pokus = ['hilft zu lernen oder.', 'njn jak nn se mas ', ' s a song by the',\n",
        "         'Существуют оба потом', 'Южнославянски език, написан на кирилица', 'ファベットブルガリア語キリル文字',\n",
        "         'slaavilainen kieli, joka', 'اللغة السلافية الجنوبية مكتوبة']\n",
        "for i in range(0, len(pokus)):\n",
        "    print(loaded_model.predict_proba(pokus)[i])\n",
        "    print(languages[loaded_model.predict_proba(pokus).argmax(-1)[i]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.01040848 0.01026844 0.00402816 0.7843419  0.002695   0.01620735\n",
            " 0.00559595 0.02351029 0.00924845 0.00482381 0.00927008 0.11960209]\n",
            "DE\n",
            "[0.48002282 0.02351725 0.0375638  0.02413878 0.01949789 0.135937\n",
            " 0.04214942 0.04339878 0.01913916 0.01364655 0.13951835 0.02147019]\n",
            "CZ\n",
            "[4.57926015e-03 1.03126948e-02 9.88920814e-04 7.94266947e-04\n",
            " 9.33383964e-01 5.37864381e-03 2.51477789e-03 2.97311970e-02\n",
            " 4.25743902e-03 2.72081517e-03 2.07491178e-03 3.26310854e-03]\n",
            "EN\n",
            "[0.05905843 0.25698127 0.04660967 0.03576053 0.03032131 0.08022452\n",
            " 0.04794622 0.25286381 0.05001107 0.03405699 0.0741988  0.03196736]\n",
            "RU\n",
            "[1.40725206e-05 1.57515026e-03 1.17495153e-05 7.75252116e-06\n",
            " 5.57067940e-06 2.04064643e-05 1.29130646e-05 2.53488522e-05\n",
            " 1.87944820e-03 1.01154519e-05 1.68807970e-05 9.96420592e-01]\n",
            "BG\n",
            "[0.07016339 0.08005213 0.05307346 0.04131687 0.03641797 0.09590707\n",
            " 0.05529808 0.32347421 0.06249334 0.04089406 0.08757327 0.05333615]\n",
            "JA\n",
            "[1.01512069e-05 1.17832362e-05 8.16780927e-06 6.24424098e-06\n",
            " 4.79296175e-06 1.35278191e-05 8.50346854e-06 3.12517471e-05\n",
            " 1.00183112e-05 8.09204114e-06 9.99876891e-01 1.05764912e-05]\n",
            "FI\n",
            "[0.0256977  0.02840599 0.02141832 0.01645981 0.01135862 0.03426179\n",
            " 0.02199852 0.07865682 0.0226087  0.68622405 0.03162199 0.02128768]\n",
            "AR\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6aIaJ1anTe5"
      },
      "source": [
        "## Save links for future documentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ize4ZCiho_A6"
      },
      "source": [
        "#https://github.com/jeffheaton/present/blob/master/youtube/read_wikipedia.ipynb\n",
        "# https://stackoverflow.com/questions/18854620/whats-the-best-way-to-split-a-string-into-fixed-length-chunks-and-work-with-the/18854817"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}